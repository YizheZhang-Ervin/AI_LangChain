import os
import dotenv
import dashscope
import redis
import numpy as np
from http import HTTPStatus
from redis.commands.search.query import Query
from openai import OpenAI

# ========== é…ç½® ==========
dotenv.load_dotenv()
dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")

INDEX_NAME = "faq_index"
VECTOR_DIM = 1024
TOP_K = 3

redis_client = redis.Redis(
    host="localhost",
    port=6379,
    password=None,
    decode_responses=False
)

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆå…¼å®¹ DashScopeï¼‰
client = OpenAI(
    api_key=os.getenv("BAILIAN_API_KEY"),
    base_url=os.getenv("BAILIAN_BASE_URL")
)

# ========== å°†é—®é¢˜è½¬ä¸ºå‘é‡ ==========
def embed_question(question: str):
    resp = dashscope.MultiModalEmbedding.call(
        model="multimodal-embedding-v1",
        input=[{"text": question}]
    )
    if resp.status_code == HTTPStatus.OK:
        embedding = resp.output["embeddings"][0]["embedding"]
        return np.array(embedding, dtype=np.float32).tobytes()
    else:
        raise RuntimeError(f"âŒ Embedding è°ƒç”¨å¤±è´¥: {resp.code}, {resp.message}")

# ========== ç›¸ä¼¼åº¦æœç´¢ ==========
def search_faq(question: str, top_k=TOP_K):
    q_vector = embed_question(question)

    query = (
        Query(f"*=>[KNN {top_k} @embedding $vec AS score]")
        .sort_by("score")
        .return_fields("question", "answer", "source", "category", "crawl_time", "score")
        .dialect(2)
    )

    results = redis_client.ft(INDEX_NAME).search(query, query_params={"vec": q_vector})
    return results.docs

# ========== æ„å»º Prompt ==========
def build_prompt(user_question: str, retrieved_docs, top_k=TOP_K) -> str:
    context_parts = []
    for i, doc in enumerate(retrieved_docs[:top_k], start=1):
        context_parts.append(
            f"ã€æ–‡æ¡£ç‰‡æ®µ{i}ã€‘\nQ: {doc.question}\nA: {doc.answer}"
        )
    context_text = "\n\n".join(context_parts)

    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ï¼Œè¯·ä»…æ ¹æ®æä¾›çš„æ–‡æ¡£ç‰‡æ®µå›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    å¦‚æœæ–‡æ¡£ç‰‡æ®µä¸­æ²¡æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·å›ç­”â€œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ã€‚
    
    ç”¨æˆ·é—®é¢˜ï¼š
    {user_question}
    
    å¯ç”¨æ–‡æ¡£ç‰‡æ®µï¼š
    {context_text}
    
    è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆç®€æ´æ˜äº†çš„å›ç­”ï¼š
    """
    return prompt.strip()

# ========== è°ƒç”¨å¤§æ¨¡å‹ ==========
def ask_llm(prompt: str) -> str:
    completion = client.chat.completions.create(
        model="deepseek-r1-distill-llama-70b",  # ä¹Ÿå¯ä»¥æ¢æˆ qwen-turbo / qwen-plus ç­‰
        messages=[{"role": "user", "content": prompt}]
    )

    # è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
    return completion.choices[0].message.content

# ========== ä¸»ç¨‹åº ==========
if __name__ == "__main__":
    while True:
        user_question = input("\nè¯·è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰ï¼š")
        if user_question.lower() in ["exit", "quit"]:
            break

        docs = search_faq(user_question, top_k=TOP_K)
        if not docs:
            print("âš ï¸ æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£")
            continue

        prompt = build_prompt(user_question, docs)
        # print("\n===== æ„å»ºçš„ Prompt =====\n")
        # print(prompt)
        # print("\n=========================\n")

        answer = ask_llm(prompt)
        print("ğŸ’¡ å¤§æ¨¡å‹å›ç­”ï¼š")
        print(answer)
